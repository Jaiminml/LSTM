{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "scenic-learning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\baps\\anaconda3\\envs\\lstm\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import matplotlib.pyplot as plt \n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hourly-nightmare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "formed-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will check the shape of the data frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-bulletin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many values are none and we have to drop it.\n",
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinated-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unusual-choir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ongoing-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have deleted all the data which had NAN values becasue it can affect the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loaded-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the independent and dependent variables\n",
    "x = df.drop('label',axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excited-namibia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "israeli-heather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "curious-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thick-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a vocabulary size \n",
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-telling",
   "metadata": {},
   "source": [
    "# One hot representation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "working-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we dropped the values we have to reset the index to make one hot representation of data\n",
    "sentences = x.copy()\n",
    "sentences.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sustainable-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we have to preprocess the text because we have many ounctuations and STOPWORDS in our dataset so we have to preprocess that data\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "indie-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "owned-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences['title'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fifteen-browser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hous dem aid even see comey letter jason chaffetz tweet',\n",
       " 'flynn hillari clinton big woman campu breitbart',\n",
       " 'truth might get fire',\n",
       " 'civilian kill singl us airstrik identifi',\n",
       " 'iranian woman jail fiction unpublish stori woman stone death adulteri',\n",
       " 'jacki mason hollywood would love trump bomb north korea lack tran bathroom exclus video breitbart',\n",
       " 'beno hamon win french socialist parti presidenti nomin new york time',\n",
       " 'back channel plan ukrain russia courtesi trump associ new york time',\n",
       " 'obama organ action partner soro link indivis disrupt trump agenda',\n",
       " 'bbc comedi sketch real housew isi caus outrag']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we can see that corpus contains the words after preprocessing done.\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "elect-victim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1076, 2004, 3855, 3964, 1251, 1177, 2432, 4548, 4821, 3157],\n",
       " [3090, 3921, 277, 3803, 561, 2494, 2349],\n",
       " [792, 2085, 3099, 206],\n",
       " [1083, 2836, 2939, 3433, 2700, 2344],\n",
       " [4308, 561, 750, 666, 2017, 2368, 561, 415, 869, 208],\n",
       " [4623,\n",
       "  3529,\n",
       "  4621,\n",
       "  2659,\n",
       "  3924,\n",
       "  4115,\n",
       "  2845,\n",
       "  2475,\n",
       "  4603,\n",
       "  4988,\n",
       "  1575,\n",
       "  959,\n",
       "  92,\n",
       "  1630,\n",
       "  2349],\n",
       " [4348, 1735, 189, 352, 3582, 757, 60, 4393, 373, 1561, 684],\n",
       " [1554, 61, 1640, 1548, 2048, 1673, 4115, 2910, 373, 1561, 684],\n",
       " [2484, 2659, 3720, 3690, 509, 4227, 4554, 310, 4115, 1168],\n",
       " [653, 1048, 646, 2146, 2026, 1062, 3558, 4097]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that the words in corpus are represented in onehot encode w.r.t. given vocab_size that is 5000\n",
    "onehot = [one_hot(words,vocab_size) for words in corpus]\n",
    "onehot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "constant-supply",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18285"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-uruguay",
   "metadata": {},
   "source": [
    "# Embedding of onehot words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "associate-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = pd.DataFrame(corpus,columns=['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "native-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df['length'] = length_df['corpus'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "documentary-excellence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_df['length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "soviet-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we have maximum sentence length is 299 so we will take embedding accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "paperback-antigua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0, 1076, 2004,\n",
       "        3855, 3964, 1251, 1177, 2432, 4548, 4821, 3157],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0, 3090, 3921,  277, 3803,  561, 2494, 2349],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  792, 2085, 3099,  206],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 1083, 2836, 2939, 3433, 2700, 2344],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0, 4308,  561,\n",
       "         750,  666, 2017, 2368,  561,  415,  869,  208],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0, 4623, 3529, 4621, 2659, 3924, 4115, 2845,\n",
       "        2475, 4603, 4988, 1575,  959,   92, 1630, 2349],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0, 4348, 1735,  189,\n",
       "         352, 3582,  757,   60, 4393,  373, 1561,  684],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0, 1554,   61, 1640,\n",
       "        1548, 2048, 1673, 4115, 2910,  373, 1561,  684],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0, 2484, 2659,\n",
       "        3720, 3690,  509, 4227, 4554,  310, 4115, 1168],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         653, 1048,  646, 2146, 2026, 1062, 3558, 4097]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 30\n",
    "embedding = pad_sequences(onehot,maxlen=length,padding='pre')\n",
    "embedding[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "linear-jonathan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 40)            200000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               112800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 313,001\n",
      "Trainable params: 313,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "embedding_vector_features = 40\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_vector_features,input_length=length))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "catholic-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(embedding)\n",
    "Y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "institutional-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "generous-experience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "192/192 [==============================] - 9s 45ms/step - loss: 0.3227 - accuracy: 0.8448 - val_loss: 0.2034 - val_accuracy: 0.9152\n",
      "Epoch 2/20\n",
      "192/192 [==============================] - 8s 39ms/step - loss: 0.1433 - accuracy: 0.9411 - val_loss: 0.1821 - val_accuracy: 0.9238\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - 8s 44ms/step - loss: 0.0897 - accuracy: 0.9668 - val_loss: 0.2037 - val_accuracy: 0.9218\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - 10s 54ms/step - loss: 0.0573 - accuracy: 0.9803 - val_loss: 0.2556 - val_accuracy: 0.9193\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - 10s 52ms/step - loss: 0.0307 - accuracy: 0.9903 - val_loss: 0.3273 - val_accuracy: 0.9158\n",
      "Epoch 6/20\n",
      "192/192 [==============================] - 9s 49ms/step - loss: 0.0170 - accuracy: 0.9956 - val_loss: 0.3483 - val_accuracy: 0.9168\n",
      "Epoch 7/20\n",
      "192/192 [==============================] - 9s 47ms/step - loss: 0.0138 - accuracy: 0.9950 - val_loss: 0.4654 - val_accuracy: 0.9099\n",
      "Epoch 8/20\n",
      "192/192 [==============================] - 10s 55ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.5041 - val_accuracy: 0.9117\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - 11s 58ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.5213 - val_accuracy: 0.9100\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - 14s 72ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.5411 - val_accuracy: 0.9079\n",
      "Epoch 11/20\n",
      "192/192 [==============================] - 14s 73ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.5063 - val_accuracy: 0.9122\n",
      "Epoch 12/20\n",
      "192/192 [==============================] - 13s 68ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.5529 - val_accuracy: 0.9087\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - 13s 65ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.5330 - val_accuracy: 0.9095\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - 13s 69ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.5599 - val_accuracy: 0.9118\n",
      "Epoch 15/20\n",
      "192/192 [==============================] - 11s 55ms/step - loss: 2.7169e-04 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 0.9099\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 1.1285e-04 - accuracy: 1.0000 - val_loss: 0.6487 - val_accuracy: 0.9097\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - 11s 58ms/step - loss: 8.3286e-05 - accuracy: 1.0000 - val_loss: 0.6669 - val_accuracy: 0.9097\n",
      "Epoch 18/20\n",
      "192/192 [==============================] - 9s 48ms/step - loss: 6.5950e-05 - accuracy: 1.0000 - val_loss: 0.6818 - val_accuracy: 0.9092\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - 9s 49ms/step - loss: 5.4030e-05 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 0.9090\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - 11s 59ms/step - loss: 4.4982e-05 - accuracy: 1.0000 - val_loss: 0.7104 - val_accuracy: 0.9094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14db8589048>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model \n",
    "model.fit(X_train,y_train,batch_size=64,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "large-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "## Creating model\n",
    "embedding_vector_features=40\n",
    "model1=Sequential()\n",
    "model1.add(Embedding(vocab_size,embedding_vector_features,input_length=length))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Bidirectional(LSTM(200)))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(1,activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "compressed-upgrade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "192/192 [==============================] - 18s 94ms/step - loss: 0.3229 - accuracy: 0.8443 - val_loss: 0.2085 - val_accuracy: 0.9026\n",
      "Epoch 2/20\n",
      "192/192 [==============================] - 21s 108ms/step - loss: 0.1551 - accuracy: 0.9378 - val_loss: 0.1851 - val_accuracy: 0.9234\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - 22s 117ms/step - loss: 0.1096 - accuracy: 0.9584 - val_loss: 0.2011 - val_accuracy: 0.9195\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - 23s 118ms/step - loss: 0.0726 - accuracy: 0.9740 - val_loss: 0.2118 - val_accuracy: 0.9213\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - 25s 132ms/step - loss: 0.0455 - accuracy: 0.9841 - val_loss: 0.2560 - val_accuracy: 0.9163\n",
      "Epoch 6/20\n",
      "192/192 [==============================] - 23s 121ms/step - loss: 0.0356 - accuracy: 0.9884 - val_loss: 0.3061 - val_accuracy: 0.9158\n",
      "Epoch 7/20\n",
      "192/192 [==============================] - 26s 133ms/step - loss: 0.0266 - accuracy: 0.9909 - val_loss: 0.3575 - val_accuracy: 0.9104\n",
      "Epoch 8/20\n",
      "192/192 [==============================] - 27s 141ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.3594 - val_accuracy: 0.9123\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - 25s 131ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.3828 - val_accuracy: 0.9148\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - 27s 140ms/step - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.4397 - val_accuracy: 0.9072\n",
      "Epoch 11/20\n",
      "192/192 [==============================] - 25s 133ms/step - loss: 0.0153 - accuracy: 0.9946 - val_loss: 0.4429 - val_accuracy: 0.9133\n",
      "Epoch 12/20\n",
      "192/192 [==============================] - 27s 140ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.5077 - val_accuracy: 0.9074\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - 25s 133ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 0.5303 - val_accuracy: 0.9132\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - 26s 136ms/step - loss: 0.0145 - accuracy: 0.9949 - val_loss: 0.4775 - val_accuracy: 0.9090\n",
      "Epoch 15/20\n",
      "192/192 [==============================] - 24s 125ms/step - loss: 0.0097 - accuracy: 0.9962 - val_loss: 0.5937 - val_accuracy: 0.9060\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - 24s 126ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.5409 - val_accuracy: 0.9070\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - 24s 126ms/step - loss: 0.0068 - accuracy: 0.9974 - val_loss: 0.5542 - val_accuracy: 0.9067\n",
      "Epoch 18/20\n",
      "192/192 [==============================] - 25s 129ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.7927 - val_accuracy: 0.9070\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - 24s 127ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.5573 - val_accuracy: 0.9105\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - 25s 129ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.6106 - val_accuracy: 0.9082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14dc8f8ff48>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model \n",
    "model1.fit(X_train,y_train,batch_size=64,epochs=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-jewelry",
   "metadata": {},
   "source": [
    "We can see that after adding dropout we are not getting accuracy more than 90% so we will take different approaches but we can see the entire process of how LSTM cab ne used in text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-cholesterol",
   "metadata": {},
   "source": [
    "# Now can check the performance of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "united-opening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-1c49758e8ce6>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "[[3122  297]\n",
      " [ 250 2366]]\n",
      "0.9093620546810274\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(X_test)\n",
    "CM = confusion_matrix(y_test,y_pred)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(CM)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-piano",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
